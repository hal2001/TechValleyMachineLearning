{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing in Keras\n",
    "\n",
    "Welcome! In this tutorial, I am going to cover a few approaches to NLP in the Keras framework. We have a **lot** of ground to cover, and I will try to cater to as broad a range of skill levels as I can manage. If you have any questions, do not hesitate to ask! I'm kind of making this up as I go along, so I am not sure how balanced the difficulty of each section is. We should have plenty of time to cover with ample questions. \n",
    "\n",
    "# What is NLP?\n",
    "\n",
    "Natural language processing, or NLP, is a branch of computer science concerned with getting machines to understand and produce human language. Human language can be very tricky. Take the following [garden path sentences](https://en.wikipedia.org/wiki/Garden_path_sentence):\n",
    "\n",
    "<br><center>**Time flies like an arrow. Fruit flies like a banana**</center>\n",
    "\n",
    "In the first sentence, *flies* is a verb. In the second, the exact same word is a noun! That is why we humans find it humorous - it throws our brain in a direction it did not expect. To get this into machine-speak, we would have to do something like:\n",
    "\n",
    "<br><center>**Time is similar to an arrow in that both move swiftly. Drosophila melanogaster enjoy consuming Musa acuminata**</center>\n",
    "\n",
    "Kinda takes the fun out of it, huh? But that is exactly what we have to do when it comes to processing natural language with computers. We can understand language, in spoken or written form, often with ambiguous context, thanks to the exaflop processor sitting in our skull. Computers have one one-millionth of that to work with, so we are going to have to be clever. \n",
    "\n",
    "# The goal\n",
    "\n",
    "Our primary task is fairly straightforward: given a written review of a movie, can we rate it as positive or negative? I am also working on a secondary challenge, which may or may not make it into the final tutorial. That challenge is, given a model trained on reviews from one source (IMDB), can we use it to accurately rate reviews from another source(Rotten Tomatoes)? The former is a toy example, which the latter has been a bit of a challenge for me! I'm experimenting as I go along, so even as I write this, I'm not sure how it'll turn out!\n",
    "\n",
    "## Hurdles\n",
    "\n",
    "There are two main hurdles in our way, and I will be covering some mainstream approaches to tackling them. The first is vocabulary: How do we take the 170,000+ words in the English language, and render that into a number which makes sense to a computer, and does not take a huge amount of information? This process is known as **embedding**. The second is context: As demonstrated in our above phrase, a word by itself is not guaranteed to have a single meaning. We have to look to the *context*, that is, the words and sentences around it. We will tackle this with **Convolutional networks** and **Recurrent networks**. \n",
    "\n",
    "Alright, let's look at some code! First, let's load in the libraries we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb, reuters\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, RepeatVector\n",
    "\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras_tqdm import TQDMNotebookCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Keras recently rolled out V2.0 with some breaking changes. I'll try to keep this tutorial as v1/v2 compatible as possible in case people are still running v1. \n",
    "If you see errors like `TypeError: Received unknown keyword arguments: {'epochs': 3}`, or size/shape mismatches, you probably have a version mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocess the dataset into suitable shape to feed to NN\n",
    "\n",
    "We will load in the data into memory, and then process it from the current form into a form that our model can accept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nb_top_words           = 5000  # Load the dataset but only keep the top n words, zero the rest. These will be our 'symbols' in our abridged corpus\n",
    "sequence_len           = 50   # This is the length in the \"time axis\". Originally was 500, but it turns out 50 works *almost* as well, and way faster!\n",
    "embed_vector_len       = 32    # Size of the feature vector each word will map to\n",
    "nb_lstm                = 100   # Number of LSTM nodes\n",
    "batch_size             = 64    # Number of samples to feed into the model for each forward/backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/datasets/imdb.py:44: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=nb_top_words, skip_top=0, index_from=3)#nb_words=nb_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "twitter_x = np.load('data/twitter_phrases.npy') # pre-converted indexes from Twitter set using IMDB word list\n",
    "twitter_y = np.load('data/twitter_labels.npy')\n",
    "rotten_x = np.load('data/rotten_phrases.npy') # pre-converted indexes from Twitter set using IMDB word list\n",
    "rotten_y = np.load('data/rotten_labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Let's peer into the data. I always like to start by getting a feel for the data\n",
    "\n",
    "This data is in the form of a sequence of integers.  Each `int` maps to a word in the corpus dictionary. It is a giant lookup table, with the index roughly corresponding to the frequency rank of the word. This is necessary because computers do not directly process words, only numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '~'),\n",
       " (1, 'the'),\n",
       " (2, 'and'),\n",
       " (3, 'a'),\n",
       " (4, 'of'),\n",
       " (5, 'to'),\n",
       " (6, 'is'),\n",
       " (7, 'br'),\n",
       " (8, 'in'),\n",
       " (9, 'it')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "index_word = {value: key for (key, value) in word_index.items()} # flip key:value pairs to get the integer as the key\n",
    "index_word.update({0: \"~\"})\n",
    "list(index_word.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example sample. Let's look at the raw data, and then the text it corresponds to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([263,   2, 182,   5,  17,  75,   2, 922,  36, 279, 131,   2,  17,\n",
       "         2,  42,  17,  35, 921,   2, 192,   5,   2,   2,  19,   2, 217,\n",
       "         2,   2, 537,   2,   2,   5, 736,  10,  10,  61, 403,   9,   2,\n",
       "        40,  61,   2,   5,  27,   2, 159,  90, 263,   2,   2, 309,   8,\n",
       "       178,   5,  82,   2,   4,  65,  15,   2, 145, 143,   2,  12,   2,\n",
       "       537, 746, 537, 537,  15,   2,   4,   2, 594,   7,   2,  94,   2,\n",
       "         2,   2,  11,   2,   4, 538,   7,   2, 246,   2,   9,   2,  11,\n",
       "       635,  14,   9,  51, 408,  12,  94, 318,   2], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_num = 3\n",
    "x_train[sample_num][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"comes and young to movie bad and dream from reason these and movie and it's movie so fi and enough to and and film and almost and and obviously and and to appears i i only human it and just only and to be and new made comes and and high in want to other and of their for and those i'm and that and obviously message obviously obviously for and of and brother br and make and and and this and of blood br and worst and it and this across as it when lines that make excellent and that there is and fantasy to and and film good br of and and and have into your whatever i i and and and be and this and and new be home all and film and lot br made and in at this of and how and in and some this and not all it and are of and and re is and and and i i worst more it is and and message made all and in does of nor of nor side be and and obviously know end and here to all tries in does of nor side of home br be indeed i i all it and in could is performance and and in of and br by br and its and and well of nor at coming it's it that an this obviously i i this as their has obviously bad and and and and and of and br work to of run up and and br and nor this early her bad having and film and movie all care of their br be right acting i i and of and and it away of its and and to and version you br and your way just and was can't and and film of and br and obviously are up obviously not other just and was and as true was least of and certainly lady poorly of setting and and br and to make just have 2 which and of and dialog and br of and say in can is you for it wasn't in and as by it away plenty what have reason and are that and that's have 2 which sister and of important br and to of took work 20 br similar more he good and for hit at coming not see and\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([index_word[idx] for idx in x_train[sample_num]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wat.\n",
    "Ok, I totally did not expect that. It seems super nonsensical. I dug into it and I think the situation is that the Keras IMDB data has already been processed with bag-of-words. I left this in here because it's important to remember that **Data science is a science**. Unexpected things happen all the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "#### Next, we need to pad the tensors out to the proper dimension in the time axis. \n",
    "Even though LSTM can handle variable length data, the backend still prefers rectangular tensors. We will pad/crop our variable-length movie reviews so that they are all exactly the same length. Once we have regularly-sized tensors, we are ready to build the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=sequence_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50) (25000, 50) (1291, 50) (3928, 50)\n"
     ]
    }
   ],
   "source": [
    "twitter_x = sequence.pad_sequences(twitter_x, maxlen=sequence_len)\n",
    "rotten_x = sequence.pad_sequences(rotten_x, maxlen=sequence_len)\n",
    "print(x_train.shape, x_test.shape, twitter_x.shape, rotten_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Without further ado, let's build an RNN in 5 lines of code. I'll walk you through each layer in detail. \n",
    "\n",
    "First, we initialize the model.\n",
    "\n",
    "> ```python\n",
    "model = Sequential()```\n",
    "\n",
    "The [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) is based around building up the model layer-by-layer, like a cake. This is the easiest to graps for beginners, and works well, since many, if not most, neural networks can be represented this way. \n",
    "Calling model.add(layer) sticks the layer onto the topmost, and that becomes the new top. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Embedding\n",
    "\n",
    "We start off with an efficient embedding layer which maps our vocabulary to a lower-dimensional space.\n",
    "\n",
    "> ```python\n",
    "model.add(Embedding(nb_top_words,                 \n",
    "                    embed_vector_len, \n",
    "                    input_length=sequence_len\n",
    "                   ))\n",
    "```\n",
    "\n",
    "#### One-hot encoding\n",
    "\n",
    "Currently, we have every word mapped to some integer, which is great because the computer can parse it, but another problem arises: These numbers are sort of arbitrary. 55 (time) and 56 (she), have absolutely no conection, even though they are numerically close. Since neural networks are essentially giant (non)linear equations, this is not ideal. Larger numbers would get more 'weight' than smaller ones. What we need to do is map it in to some sort of categorical representation, where each word is on 'equal footing'. \n",
    "\n",
    "We achieve this by using a [*one hot* representation](https://www.quora.com/What-is-one-hot-encoding-and-when-is-it-used-in-data-science). This is where you have an Nx1 vector, where the dimension associated with the integer has value 1 ('hot') and all other are 0. N is the size of your vocabulary. Each word would look like this:\n",
    "\n",
    "[0,0,0,0,0,0,0,...,0,**1**,0,0,...,0,0,0]\n",
    "<br>        ^ this is the $kth$ dim, where $k$ corresponds to the index in the table\n",
    "\n",
    "\n",
    "Here is a toy example:\n",
    "\n",
    "```\n",
    "here    = [0,0,0,0,1,0]\n",
    "is      = [0,1,0,0,0,0]\n",
    "a       = [1,0,0,0,0,0]\n",
    "toy     = [0,0,0,1,0,0]\n",
    "example = [0,0,0,0,0,1]\n",
    "```\n",
    "\n",
    "Normally, you would have way more dimensions (typically thousands). Fortunately, Keras takes care of this automatically. We give it a sequence of M integers, and it'll automatically convert it to a NxM matrix.\n",
    "\n",
    "Currently, the \"shape\" of each data vector is Nx1, where N = 5000. That's really problematic, due to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) and also computational constraints. We want to transform our **uncondensed** and **sparse** data into a **compact** and **dense** representation, in our case a 32-vector. This process is known as embedding. **Word2Vec** is a popular word embedding system. Keras Embedding layer works very similarly, so if you want to know more, I suggest checking out how Word2Vec works. \n",
    "\n",
    "After embedding, we will have a much more manageable 32x50 matrix. \n",
    "\n",
    "### LSTM\n",
    "\n",
    "> ```python\n",
    "model.add(LSTM(nb_lstm))```\n",
    "\n",
    "\n",
    "The LSTM (long short-term memory) cell is a neuron with memory. It accomplishes this by have a memory state, which can be written to and read from. It's like a tiny ~~casette tape~~  [~~floppy disk~~](http://i.imgur.com/Osxo1UF.jpg)  USB flash drive. In short, the inputs from the prior layer (mathematically) control gates. These gates determine whether to erase, write, and/or read from the memory cell. LSTMs have been covered really well in depth  in a lot of places. In particular I recommend the articles by [colah](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) and [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
    "\n",
    "![LSTM](http://tc.sinaimg.cn/maxwidth.800/tc.service.weibo.com/cdn_images_1_medium_com/58ad765e09eacb5116c9dfc5897c7296.png)\n",
    "\n",
    "### Dense\n",
    "\n",
    "> ```python\n",
    "model.add(Dense(1, activation='sigmoid'))```\n",
    "\n",
    "\n",
    "Finally, we have a densely-connected layer. This is your \"typical\" neural network layer - each node from the prior layer connects to each node of the following. In this case, we are crunching down to a single node since we want a single answer - \"Positive\" or \"Negative\". We'll use a sigmoid activation to squash the output to the range 0-1. \n",
    "\n",
    "That's it for our network! Pretty simple, right? \n",
    "\n",
    "> [\"Keras is so good that it is effectively cheating in machine learning\"](https://news.ycombinator.com/item?id=13872670)\n",
    "\n",
    "### Compiling \n",
    "\n",
    "> ```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])```\n",
    "\n",
    "All that's left is compiling the model. This tells the model which loss function, optimizer, and metrics to use. \n",
    "\n",
    "- **Optimizer**: This is the algorithm which describes how we approach gradient descent. Adam is pretty modern and works quite well for a lot of problems, so it is typically the first go-to when picking hyperparameters\n",
    "- **Metrics**: This does not actually affect the direct training of the model. Rather, it gives us humans a way to track the performance of the model over time. This can also be used for automatically early-stopping to avoid overfitting. \n",
    "- **Loss function**: This determines how the \"penalty\" for incorrect predictions is calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Brief tangent: Cross-entropy\n",
    "I'm working on a [simple summary of Cross-Entropy](https://github.com/xkortex/TechValleyMachineLearning/blob/master/CrossEntropy.ipynb). If you would like to know more, then check out that post, otherwise it's a bit of a tangent for this particular project. \n",
    "\n",
    "For now, all we need to know is cross-entropy is a very common loss function, and many Keras models use binary (yes/no problems) or categorical (multiple labels) cross-entropy. You are probably familiar with Mean Squared Error, which is a commonly used loss function if you are performing continuous regression. Cross-entropy is used to predict labels (logistic regression). The legendary [Andrew Ng Coursera Course](https://www.coursera.org/learn/machine-learning) covers this in more detail. \n",
    "\n",
    "# Let's write the code for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_top_words,                 \n",
    "                    embed_vector_len, \n",
    "                    input_length=sequence_len\n",
    "                   ))\n",
    "model.add(LSTM(nb_lstm))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 32)            160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301.0\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Have a look at our model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the model to the data\n",
    "This is where the magic happens! The Tensor Gnomes will do their ritual dance, and the weights will manifest new values.\n",
    "\n",
    "...What do you mean, that's not how it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0a4e4fbae04385acefa38c201046f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882e36d73023493f91e00dbd20874565"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60eb91cf733646ae857c1972bd23ae42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b227bf180c14ae19fc9312d167310f1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This will take about 20-60 seconds per epoch on a GPU. CPU will take anywhere from 30s (i7 w/ 8 threads) to several minutes per epoch. \n",
    "# Larger networks will see a bigger advantage to GPU, though CPU did not do as poorly as I expected!\n",
    "# We'll be making improvements to the model, so you don't have to run this just yet.\n",
    "RUN_MODEL1 = True# I'm just using a switch here so I can run Kernel -> Restart and Run All\n",
    "if RUN_MODEL1: \n",
    "    model.fit(x_train, y_train, \n",
    "              validation_data=(x_test, y_test), \n",
    "              epochs=3, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, # Some versions of Jupyter bork on Keras' progress bar. We replace it with Keras-TQDM instead\n",
    "              callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This network will get us to about 87% accuracy. However, we had to stop pretty early because of the risk of overfitting. A model that overfits easy is often a strong sign that the model will generalize poorly to new, unseen data, or data from different distributions. Let's see how it performs on a similar dataset, the Twitter dataset. This will be challenging, as it is much shorter, which gives the RNN less time to 'get up to speed'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1291 [==========================>...] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bLabel ratio: 56.70%\n",
      "Loss: 1.5329099910190915\n",
      "Metric: 45.31%\n"
     ]
    }
   ],
   "source": [
    "if RUN_MODEL1:\n",
    "    loss, metric = model.evaluate(twitter_x, twitter_y)\n",
    "    print('Label ratio: {:.2f}%'.format(np.mean(twitter_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Ouch. No better than chance.\n",
    "Let's see if we can do a bit better with the generalization. Since the dataset we want to extrapolate to (Twitter, Rotten Tomatoes, etc) is different in several ways, we want our IMDB-trained network to be robust to noise, idiosyncracies, and quirks unique to IMDB that do not generalize to other formats.  \n",
    "\n",
    "### Adding dropout and noise\n",
    "\n",
    "**Dropout** is a technique for reducing overfitting in neural networks by preventing over-adaptation to the training data. The general idea is, every cycle you randomly drop a certain percentage of nodes or connections. This forces the network to compensate by distributing over multiple nodes and prevents any given node from getting too \"specialized\". \n",
    "\n",
    "Adding noise is another way of reducing overfitting and improving generalization. By adding noise to the vectors, this forces the network to learn to compensate, just as humans process stimuli in a noisy environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.2 # Rate of input units to drop\n",
    "sigma=0.5         # Amount of noise to add (in terms of standard deviation)\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(nb_top_words,                 \n",
    "                    embed_vector_len, \n",
    "                    input_length=sequence_len\n",
    "                   ))\n",
    "model.add(GaussianNoise(sigma=sigma))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(nb_lstm))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This will take about 4-6 minutes per epoch on a GPU. \n",
    "RUN_MODEL2 = False\n",
    "if RUN_MODEL2:\n",
    "    model.fit(x_train, y_train, \n",
    "              validation_data=(x_test, y_test), \n",
    "              nb_epoch=3, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, # Some versions of Jupyter bork on Keras' progress bar. We replace it with Keras-TQDM instead\n",
    "              callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if RUN_MODEL2:\n",
    "    loss, metric = model.evaluate(twitter_x, twitter_y)\n",
    "    print('Label ratio: {:.2f}%'.format(np.mean(twitter_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# LSTM + CNN = REAL ULTIMATE POWER!\n",
    "\n",
    "#### LSTMs are awesome. Convolutional neural networks are awesome. I was blown away when I learned you can simply and easily combine both into the same model!\n",
    "\n",
    "Convolutional layers look at local structures in the data. In image classifiers, this is visual features. In NLP, this looks at groups of words, or N-grams (sequence of N words). For instance, many English sentences contain N-grams of the form [**subject verb object**]. For example (dropping articles for simplicity):\n",
    "\n",
    "- John rode bike\n",
    "- Suzie hit ball\n",
    "- Bobby made memes\n",
    "\n",
    "Another common sequence is [**subject copula predicate**] (copula verbs: is, are, was, will be: \n",
    "\n",
    "- Roses are red\n",
    "- Movie was bad\n",
    "- Keras is awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:9: UserWarning: Update your `GaussianNoise` call to the Keras 2 API: `GaussianNoise(stddev=0.5)`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:10: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(kernel_size=5, padding=\"same\", filters=256, activation=\"relu\")`\n",
      "/home/mike/ve/keras/lib/python3.5/site-packages/ipykernel/__main__.py:11: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    }
   ],
   "source": [
    "nb_filter = 256     # This is the number of convolutional filters to use\n",
    "filter_length = 5  # This is the size of the filter kernel. Since this is 1D, the kernel is Nx1\n",
    "pool_length = 2    # Size of our max pooling structures\n",
    "dropout_rate = 0.2 # Rate of input units to drop\n",
    "sigma=0.5         # Amount of noise to add (in terms of standard deviation)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_top_words, embed_vector_len, input_length=sequence_len))\n",
    "model.add(GaussianNoise(sigma=sigma))\n",
    "model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 32)            160000    \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 256)           41216     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               142800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 344,117.0\n",
      "Trainable params: 344,117.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## some stuff about sttructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/models.py:826: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febc4f1af97944d4896f30f069263925"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220025756df449b4bf1834f0df3a2359"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3206b216576640a89785d100b0c1ea16"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c58c707c6e4212bdf216d742172e43"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final validation accuracy: 81.49%\n"
     ]
    }
   ],
   "source": [
    "# This will take about 0.5-2 minutes per epoch on a GPU. \n",
    "RUN_MODEL3 = True\n",
    "if RUN_MODEL3:\n",
    "    model.fit(x_train, y_train, \n",
    "              validation_data=(x_test, y_test), \n",
    "              nb_epoch=3, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, # Some versions of Jupyter bork on Keras' progress bar. We replace it with Keras-TQDM instead\n",
    "              callbacks=[TQDMNotebookCallback()])\n",
    "    \n",
    "loss, metric = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Final validation accuracy: {:.2f}%'.format(metric*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152/1291 [=========================>....] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bLabel ratio: 56.70%\n",
      "Loss: 1.5329099910190915\n",
      "Metric: 45.31%\n",
      "3808/3928 [============================>.] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bLabel ratio: 52.14%\n",
      "Loss: 0.9985020602550623\n",
      "Metric: 53.39%\n"
     ]
    }
   ],
   "source": [
    "if RUN_MODEL3:\n",
    "    loss, metric = model.evaluate(twitter_x, twitter_y)\n",
    "    print('Label ratio: {:.2f}%'.format(np.mean(twitter_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))\n",
    "    loss, metric = model.evaluate(rotten_x, rotten_y)\n",
    "    print('Label ratio: {:.2f}%'.format(np.mean(rotten_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nb_top_words           = 20000  # Load the dataset but only keep the top n words, zero the rest. These will be our 'symbols' in our abridged corpus\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=nb_top_words, skip_top=0, index_from=3)#nb_words=nb_top_words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=sequence_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(nb_top_words, embed_vector_len, input_length=sequence_len))\n",
    "model.add(GaussianNoise(sigma=sigma))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(LSTM(nb_lstm))\n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RUN_MODEL4 = True\n",
    "if RUN_MODEL4:\n",
    "    model.fit(rotten_x, rotten_y, \n",
    "              validation_data=(x_test, y_test), \n",
    "              nb_epoch=10, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, # Some versions of Jupyter bork on Keras' progress bar. We replace it with Keras-TQDM instead\n",
    "              callbacks=[TQDMNotebookCallback()])\n",
    "    \n",
    "loss, metric = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Final validation accuracy: {:.2f}%'.format(metric*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, \n",
    "              validation_data=(x_test, y_test), \n",
    "              nb_epoch=10, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, # Some versions of Jupyter bork on Keras' progress bar. We replace it with Keras-TQDM instead\n",
    "              callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if RUN_MODEL4:\n",
    "    loss, metric = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('IMDB:\\nLabel ratio: {:.2f}%'.format(np.mean(twitter_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))\n",
    "    loss, metric = model.evaluate(twitter_x, twitter_y)\n",
    "    print('Twitter:\\nLabel ratio: {:.2f}%'.format(np.mean(twitter_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))\n",
    "    loss, metric = model.evaluate(rotten_x, rotten_y)\n",
    "    print('Rotten Tomatoes:\\nLabel ratio: {:.2f}%'.format(np.mean(rotten_y*100)))\n",
    "    print('Loss: {}\\nMetric: {:.2f}%'.format(loss, metric*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = nb_top_words\n",
    "latent_dim = 24\n",
    "inputs = Input(shape=(sequence_len, input_dim))\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "decoded = RepeatVector(sequence_len)(encoded)\n",
    "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
    "sequence_autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(sequence_autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# References\n",
    "\n",
    "## Datasets\n",
    "\n",
    "http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "http://snap.stanford.edu/data/web-Amazon.html\n",
    "\n",
    "## Papers\n",
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
    "- [Semi-supervised Sequence Learning](https://arxiv.org/abs/1511.01432)\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "## Code\n",
    "\n",
    "- [Keras Examples: IMDB CNN](https://github.com/fchollet/keras/blob/master/examples/imdb_cnn.py)\n",
    "- [Machine Learning Mastery: Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)\n",
    "- [Microway: Building a Movie Review Sentiment Classifier using Keras and Theano Deep Learning Frameworks](https://www.microway.com/hpc-tech-tips/keras-theano-deep-learning-frameworks/)\n",
    "- [Using  Pre-trained Word Embeddings in Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)\n",
    "\n",
    "## Tangentally related\n",
    "\n",
    "- [McCormick: Word2vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus feature: Pure CNN Model\n",
    "\n",
    "I was not sure how much ground I would be able to cover, so this section is here in case people are having a great time, and are really thirsty for knowledge! This is a pure-CNN based model, and works by essentially dumping all the N-grams together (global pooling). It does not care about the respective order of N-grams. And yet, it works quite well! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/ve/keras/lib/python3.5/site-packages/keras/datasets/imdb.py:44: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n"
     ]
    }
   ],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.89 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 1000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 50)           50000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 398, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 150,751.0\n",
      "Trainable params: 150,751.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6629e8cda74ee0b6f10e9a8ea38f8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8771ab67427e448a8c9caa35c5e345eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff6d886cb5f4966be06c9b4bdc6cf5d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa904261fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test), verbose=0, callbacks=[TQDMNotebookCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
