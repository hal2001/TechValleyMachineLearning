{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing in Keras, Part 1\n",
    "\n",
    "Welcome! In this tutorial, I am going to cover a few approaches to NLP in the Keras framework. We have a **lot** of ground to cover, and I will try to cater to as broad a range of skill levels as I can manage. If you have any questions, do not hesitate to ask! I'm kind of making this up as I go along, so I am not sure how balanced the difficulty of each section is. We should have plenty of time to cover with ample questions. \n",
    "\n",
    "This is a living document, so pardon the rough edges!\n",
    "\n",
    "# What is NLP?\n",
    "\n",
    "Natural language processing, or NLP, is a branch of computer science concerned with getting machines to understand and produce human language. Human language can be very tricky. Take the following [garden path sentences](https://en.wikipedia.org/wiki/Garden_path_sentence):\n",
    "\n",
    "<br><center>**Time flies like an arrow. Fruit flies like a banana**</center>\n",
    "\n",
    "In the first sentence, *flies* is a verb. In the second, the exact same word is a noun! That is why we humans find it humorous - it throws our brain in a direction it did not expect. To get this into machine-speak, we would have to do something like:\n",
    "\n",
    "<br><center>**Time is similar to an arrow in that both move swiftly. Drosophila melanogaster enjoy consuming Musa acuminata**</center>\n",
    "\n",
    "Kinda takes the fun out of it, huh? But that is exactly what we have to do when it comes to processing natural language with computers. We can understand language, in spoken or written form, often with ambiguous context, thanks to the exaflop processor sitting in our skull. Computers have one one-millionth of that to work with, so we are going to have to be clever. \n",
    "\n",
    "# The goal\n",
    "\n",
    "Our primary task is fairly straightforward: given a written review of a movie, can we rate it as positive or negative? I am also working on a secondary challenge, which may or may not make it into the final tutorial. That challenge is, given a model trained on reviews from one source (IMDB), can we use it to accurately rate reviews from another source(Rotten Tomatoes)? The former is a toy example, which the latter has been a bit of a challenge for me! I'm experimenting as I go along, so even as I write this, I'm not sure how it'll turn out!\n",
    "\n",
    "## Hurdles\n",
    "\n",
    "There are two main hurdles in our way, and I will be covering some mainstream approaches to tackling them. The first is vocabulary: How do we take the 170,000+ words in the English language, and render that into a number which makes sense to a computer, and does not take a huge amount of information? This process is known as **embedding**. The second is context: As demonstrated in our above phrase, a word by itself is not guaranteed to have a single meaning. We have to look to the *context*, that is, the words and sentences around it. We will tackle this with **Convolutional networks** and **Recurrent networks**. \n",
    "\n",
    "## what wil they be able to do after the journey? important in any training\n",
    "\n",
    "### how this is relevant.\n",
    "\n",
    "break up into chapters\n",
    "\n",
    "dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequence](https://image.slidesharecdn.com/2-sentimentav0-140117023306-phpapp02/95/big-data-sentiment-analysis-3-638.jpg?cb=1389926058)\n",
    "\n",
    "# How are computers going to understand sentiment? \n",
    "\n",
    "Computers only \"understand\" numbers. We need a way of converting words, concepts, ideas, and abstractions, into numerical form. Ideally, these numbers are not arbitrary, but convey something about the thing they represent. Just like the points (11,23) and (12,24) are close, wouldn't it be be great if similar concepts were \"close\"? This is the goal of the thought vector. \n",
    "\n",
    "![wat is thought vector](images/morpheus_thought.jpg)\n",
    "\n",
    "## Thought vectors are a way of representing (encoding) ideas as a vector (collection of scalars). Computers readily process vectors (graphics, video game physics, etc) so this will be useful for processing. \n",
    "\n",
    "![thought vector](images/thought_vector.png)\n",
    "\n",
    "> ...there's way too much information to decode the Matrix. You get used to it, though. Your brain does the translating. I don't even see the code. All I see is blonde, brunette, redhead. \n",
    "\n",
    "## I'm going to make the argument that the 'digital rain' in the Matrix is actually a representation of thought vectors. These vectors are very dense, whereas language is quite sparse. We will start off with a sparse representation, but later, our model will condense it for us.\n",
    "\n",
    "![digital rain](images/matrix_rain.png)\n",
    "\n",
    "## Ideally, these vectors will have this 'closeness' property we are after. \n",
    "\n",
    "![Socher-Bilingual-tsne](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/img/Socher-BillingualTSNE.png)\n",
    "<center> t-SNE visualization of the bilingual word embedding. Green is Chinese, Yellow is English. (Socher et al. (2013a))</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The first part of this demo will convert the document to a (sparse) numerical representation\n",
    "\n",
    "![flow 1](images/nlp01.png)\n",
    "\n",
    "# The second part will use that numerical represntation to train a model, which will be used to predict sentiment from other reviews\n",
    "\n",
    "![flow 2](images/nlp02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Dependencies\n",
    "\n",
    "This tutorial requires the following dependencies:\n",
    "\n",
    "#### Necessary\n",
    "- Python 3.5\n",
    "- pip - Python package manager\n",
    "- Jupyter - Web based Notebook GUI\n",
    "- Numpy - Numerical operations on arrays\n",
    "- Pandas - Manipulation of DataFrames (think excel sheets)\n",
    "- Scikit-Learn (sklearn) - Machine learning - document processing\n",
    "- Keras - Deep learning models made easy\n",
    "  - Keras will automatically install Tensorflow and Theano, underlying deep learning processing\n",
    "\n",
    "\n",
    "#### Optional\n",
    "- Virtualenv - Keep your python environments isolated\n",
    "- NLTK - Natural language toolkit - for stopword dictionary. Also useful all-around for NLP\n",
    "- tqdm  - Progress bars (*taqadum* is Arabic for \"progress\")\n",
    "- keras-tqdm - Progress bars for Keras\n",
    "\n",
    "#### Really basic setup \n",
    "\n",
    "Ideally, first you want to create a virtualenv. You can do this from a console (*nix*, OSX, sorry Windows! Not sure how to do this on Win)\n",
    "This will create a folder 've', create a virtual env named 'keras' in that, and activate the virtual environment.\n",
    "\n",
    "```bash\n",
    "mkdir ~/ve && cd ~/ve\n",
    "virtualenv -p `which python3` keras\n",
    "source ~/ve/keras/bin/activate\n",
    "```\n",
    "\n",
    "#### Install essentials\n",
    "With keras virtualenv active, \n",
    "```bash\n",
    "pip install numpy scipy pandas jupyter scikit-learn tensorflow keras nltk tqdm keras-tqdm\n",
    "```\n",
    "\n",
    "If you have CUDA set up, you can try `tensorflow-gpu` instead of `tensorflow`. But this can be a headache so I do not want to get caught up on this. \n",
    "\n",
    "### All of these packages are free! Even for commercial use! Isn't technology awesome?\n",
    "\n",
    "We will also need the source data, which I have mirrored here:\n",
    "[NLP Data (dropbox)](https://www.dropbox.com/s/hu6mjaca9zkgmr8/nlp_data.zip?dl=0)\n",
    "\n",
    "#### Let's import some libraries and get to coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nb_top_words = 5000  # Number of words to keep in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_path = '/media/mike/tera/data/nlp/techvalley/' # Point this to the path to where the CSV data is unzipped to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# where did data come from?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path + 'imdb_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After watching the Next Action Star reality TV...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a bit conflicted over this. The show is on...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I originally reviewed this film on Amazon abou...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The violent and rebel twenty-five years old sa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello. i just watched this movie earlier today...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  After watching the Next Action Star reality TV...        1.0\n",
       "1  I'm a bit conflicted over this. The show is on...        1.0\n",
       "2  I originally reviewed this film on Amazon abou...        1.0\n",
       "3  The violent and rebel twenty-five years old sa...        1.0\n",
       "4  hello. i just watched this movie earlier today...        1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is one of the most irritating, nonsensical movies I\\'ve ever had the misfortune to sit through. Every time it started to look like it might be getting good, out come more sepia tone flashbacks, followed by paranoid idiocy masquerading as social commentary. The main character, Maddox, is a manipulative, would-be rebel who lives in a mansion seemingly without any parents or responsibility. The supporting cast are all far more likeable and interesting, but are unfortunately never developed. Nor do we ever really understand the John Stanton character supposedly influencing Maddox to commit the acts of rebellion. At one point, I thought \"Aha! Maddox is just nuts and is secretly making up all those communications from escaped mental patient Stanton! Now we\\'re getting somewhere!\" but of course, that ends up to not be the case and the whole movie turns out to be pointless, both from Maddox\\'s perspective and the viewer\\'s. Where\\'s Ferris Bueller when we need him?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['review'].iloc[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# what are stopwords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mike/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download the stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# stopwords.append('br') \n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unstopper(toklist, stoplist=None):\n",
    "    \"\"\"Remove all stopwords from the sentence. Takes a list of tokens (a split string)\n",
    "    Make a list of words only if they are not in the stoplist, and then join it back together.\"\"\"\n",
    "    toklist = [w for w in toklist if not w in stoplist]\n",
    "    wordstr = ' '.join(toklist)\n",
    "    return wordstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Unleash the power of PANDAS! This is technically one line of code. But since I dislike perl-esque unreadable one-liners, I've split each operation to its own line\n",
    "\n",
    "train_phrases = train['review']\\\n",
    "                .str.replace(r'<br \\/>', ' ')\\\n",
    "                .str.replace(r'[^a-zA-Z]', ' ')\\\n",
    "                .str.lower()\\\n",
    "\n",
    "if False:\n",
    "    train_phrases = train_phrases.str.split()\\\n",
    "                    .apply(unstopper, stoplist=stopwords)\n",
    "    \n",
    "# Note the use of backslash to split to multiple lines for readability\n",
    "# Remove linebreak <br /> tags\n",
    "# Replace all non-alphabetic with spaces\n",
    "# Lowercase only\n",
    "# Tokenize\n",
    "# Remove all stopwords\n",
    "\n",
    "\n",
    "#.str.replace(r' +', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'after watching the next action star reality tv series  i was pleased to see the winners  movie right away  i was leery of such a showcase of new talent  but i was pleasantly surprised and thrilled  billy zane  of course  was his usual great self  but corinne and sean held their own beside him  it was also nice to see jared and jeanne  also from the competition  in their cameo roles  sean s character  not billy s  is the hunted  and his frustration at discovering new rules in the game is well played  corinne walks the tightrope well between her character liking sean s and only being in it for the money  i loved how the game was played right to the last second  and then beyond  not a great movie  but an entertaining one all the way and a great showcase for two folks on their first time out of the gate '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the data after processing\n",
    "train_phrases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "# Now we need to concatenate all the reviews into a single list so we can apply Bag of Words. \n",
    "big_list_train_phrases = []\n",
    "for sentence in train_phrases.values:\n",
    "    big_list_train_phrases.append(sentence)\n",
    "print(len(big_list_train_phrases))\n",
    "# big_list_train_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Numerical representation  (Vectorization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = nb_top_words - 1) \n",
    "\n",
    "# We have to subtract one in order to make room for the null character. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4999)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = vectorizer.fit_transform(big_list_train_phrases)\n",
    "train_data_features = train_data_features.toarray() # Convert to array for easier handling (this may be intense on memory)\n",
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "freq = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abc</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abilities</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ability</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>able</td>\n",
       "      <td>1259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab  freq\n",
       "0  abandoned   187\n",
       "1        abc   125\n",
       "2  abilities   108\n",
       "3    ability   454\n",
       "4       able  1259"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vocab = pd.DataFrame(list(zip(vocab, freq)), columns=['vocab', 'freq'])\n",
    "df_vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>336758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>164143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>145867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>135724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>107337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it</td>\n",
       "      <td>96472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>93981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this</td>\n",
       "      <td>76007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that</td>\n",
       "      <td>73287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>48209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vocab    freq\n",
       "1    the  336758\n",
       "2    and  164143\n",
       "3     of  145867\n",
       "4     to  135724\n",
       "5     is  107337\n",
       "6     it   96472\n",
       "7     in   93981\n",
       "8   this   76007\n",
       "9   that   73287\n",
       "10   was   48209"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by frequency rank\n",
    "df_vocab = df_vocab.sort_values(by='freq', ascending=False)\n",
    "df_vocab.reset_index(drop=True, inplace=True)\n",
    "df_vocab.index = df_vocab.index + 1   # We need to increase this to make room for our null character\n",
    "df_vocab.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Replace words with integers\n",
    "Now that we have our vocabulary, we have to create a lookup table (simple dictionary) to replace each word with an integer. Use '0' for words not in the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Invert word/int pairs to get our lookup with word as the key\n",
    "vocab_idx = {key:value for (key, value) in zip(df_vocab['vocab'], df_vocab.index)}\n",
    "\n",
    "def words_to_index(wordlist, vocab=None):\n",
    "    \"\"\"Minifunction for pandas.apply(). Replaces each word with respective index. If it's not in the vocab, replace with 0\"\"\"\n",
    "    return [vocab[word] if word in vocab else 0 for word in wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [82, 4818, 4428, 2987, 39, 4152, 3544, 4608, 3...\n",
       "1    [0, 0, 0, 437, 0, 3133, 4454, 4428, 3950, 2322...\n",
       "2    [0, 3114, 0, 4454, 1676, 3088, 0, 5, 3107, 498...\n",
       "3    [4428, 4749, 166, 3554, 4610, 1704, 4981, 3082...\n",
       "4    [2045, 0, 2400, 4816, 4454, 2902, 1358, 4507, ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split each review and replace each word with an integer\n",
    "train_idx = train_phrases.str.split().apply(words_to_index, vocab=vocab_idx)\n",
    "train_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Here is the whole process wrapped up in functions for convenience\n",
    "\n",
    "def get_vocab_index(data_phrases, verbose=False):\n",
    "    \"\"\" Proccess an array-like of strings and generate Bag of Words vocab index\n",
    "    \"\"\"\n",
    "    big_list_phrases = []\n",
    "    for sentence in data_phrases.values:\n",
    "        big_list_phrases.append(sentence)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                         tokenizer = None,    \\\n",
    "                         preprocessor = None, \\\n",
    "                         stop_words = None,   \\\n",
    "                         max_features = 5000) \n",
    "\n",
    "    if verbose: print('Vectorizing')\n",
    "    data_features = vectorizer.fit_transform(big_list_phrases)\n",
    "    data_features = data_features.toarray()\n",
    "    freq = np.sum(train_data_features, axis=0)\n",
    "\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    df_vocab = pd.DataFrame(list(zip(vocab, freq)), columns=['vocab', 'freq'])\n",
    "    df_vocab = df_vocab.sort_values(by='freq', ascending=False)\n",
    "    df_vocab.reset_index(drop=True, inplace=True)\n",
    "    df_vocab.index = df_vocab.index + 1   # We need to increase this to make room for our null character\n",
    "    vocab_idx = {key:value for (key, value) in zip(df_vocab['vocab'], df_vocab.index)}\n",
    "    return vocab_idx\n",
    "    \n",
    "\n",
    "def load_and_process_imdb_csv(file, vocab_idx=None, stopwords=None, header='infer', delimiter=None, quoting=0, show_proccessed=False, verbose=False):\n",
    "    if verbose: print('Loading')\n",
    "    data = pd.read_csv(file, header=header, delimiter=delimiter, quoting=quoting)\n",
    "    if verbose: print('Preprocesing')\n",
    "    data_phrases = data['review'].str.replace(r'<br \\/>', ' ')\\\n",
    "                    .str.replace(r'[^a-zA-Z]', ' ').str.lower()\n",
    "\n",
    "    if stopwords:\n",
    "        data_phrases = data_phrases.str.split()\\\n",
    "                        .apply(unstopper, stoplist=stopwords)\n",
    "            \n",
    "    if vocab_idx is None:\n",
    "        vocab_idx = get_vocab_index(data_phrases, verbose=verbose)\n",
    "\n",
    "    if verbose: print('Indexing')\n",
    "    data_idx = data_phrases.str.split().apply(words_to_index, vocab=vocab_idx)\n",
    "    data['vectors'] = data_idx\n",
    "    if show_proccessed:\n",
    "        data['proccessed'] = data_phrases\n",
    "    return data, vocab_idx\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Process IMDB train/test, and Rotten Tomatoes\n",
    "Now that we have our vocabulary, we can process all of our datasets. We need to use the same vocabulary in order to get reliable results. I really should do the bag of words on the full dataset, but I am running low on time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Preprocesing\n",
      "Vectorizing\n",
      "Indexing\n"
     ]
    }
   ],
   "source": [
    "train_data, vocab_idx = load_and_process_imdb_csv(data_path + 'imdb_train.csv', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After watching the Next Action Star reality TV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[96, 1038, 723, 317, 196, 4905, 897, 3767, 603...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a bit conflicted over this. The show is on...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 217, 0, 4734, 2007, 723, 4584, 1057,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I originally reviewed this film on Amazon abou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 4796, 0, 2007, 785, 274, 0, 39, 4837, 4528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The violent and rebel twenty-five years old sa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[723, 3077, 2, 2226, 1402, 4245, 4528, 894, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello. i just watched this movie earlier today...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[329, 0, 1306, 3590, 2007, 94, 385, 3853, 3652...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  After watching the Next Action Star reality TV...        1.0   \n",
       "1  I'm a bit conflicted over this. The show is on...        1.0   \n",
       "2  I originally reviewed this film on Amazon abou...        1.0   \n",
       "3  The violent and rebel twenty-five years old sa...        1.0   \n",
       "4  hello. i just watched this movie earlier today...        1.0   \n",
       "\n",
       "                                             vectors  \n",
       "0  [96, 1038, 723, 317, 196, 4905, 897, 3767, 603...  \n",
       "1  [0, 0, 0, 217, 0, 4734, 2007, 723, 4584, 1057,...  \n",
       "2  [0, 4796, 0, 2007, 785, 274, 0, 39, 4837, 4528...  \n",
       "3  [723, 3077, 2, 2226, 1402, 4245, 4528, 894, 0,...  \n",
       "4  [329, 0, 1306, 3590, 2007, 94, 385, 3853, 3652...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Preprocesing\n",
      "Indexing\n"
     ]
    }
   ],
   "source": [
    "test_data, _ = load_and_process_imdb_csv(data_path + 'imdb_test.csv', vocab_idx=vocab_idx, show_proccessed=1,  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Preprocesing\n",
      "Indexing\n"
     ]
    }
   ],
   "source": [
    "rotten_data, _ = load_and_process_imdb_csv(data_path + 'rotten.csv', vocab_idx=vocab_idx, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vectors</th>\n",
       "      <th>proccessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After watching the Next Action Star reality TV...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[96, 1038, 723, 317, 196, 4905, 897, 3767, 603...</td>\n",
       "      <td>after watching the next action star reality tv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm a bit conflicted over this. The show is on...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 0, 0, 217, 0, 4734, 2007, 723, 4584, 1057,...</td>\n",
       "      <td>i m a bit conflicted over this  the show is on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I originally reviewed this film on Amazon abou...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[0, 4796, 0, 2007, 785, 274, 0, 39, 4837, 4528...</td>\n",
       "      <td>i originally reviewed this film on amazon abou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The violent and rebel twenty-five years old sa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[723, 3077, 2, 2226, 1402, 4245, 4528, 894, 0,...</td>\n",
       "      <td>the violent and rebel twenty five years old sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hello. i just watched this movie earlier today...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[329, 0, 1306, 3590, 2007, 94, 385, 3853, 3652...</td>\n",
       "      <td>hello  i just watched this movie earlier today...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment  \\\n",
       "0  After watching the Next Action Star reality TV...        1.0   \n",
       "1  I'm a bit conflicted over this. The show is on...        1.0   \n",
       "2  I originally reviewed this film on Amazon abou...        1.0   \n",
       "3  The violent and rebel twenty-five years old sa...        1.0   \n",
       "4  hello. i just watched this movie earlier today...        1.0   \n",
       "\n",
       "                                             vectors  \\\n",
       "0  [96, 1038, 723, 317, 196, 4905, 897, 3767, 603...   \n",
       "1  [0, 0, 0, 217, 0, 4734, 2007, 723, 4584, 1057,...   \n",
       "2  [0, 4796, 0, 2007, 785, 274, 0, 39, 4837, 4528...   \n",
       "3  [723, 3077, 2, 2226, 1402, 4245, 4528, 894, 0,...   \n",
       "4  [329, 0, 1306, 3590, 2007, 94, 385, 3853, 3652...   \n",
       "\n",
       "                                          proccessed  \n",
       "0  after watching the next action star reality tv...  \n",
       "1  i m a bit conflicted over this  the show is on...  \n",
       "2  i originally reviewed this film on amazon abou...  \n",
       "3  the violent and rebel twenty five years old sa...  \n",
       "4  hello  i just watched this movie earlier today...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment5</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 603, 118, 0, 0, 723, 0, 1544, 816, 1057, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[2007, 4923, 0, 2, 692, 2667, 1057, 3497, 4748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[2064, 752, 118, 0, 0, 0, 921, 0, 2984, 566, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 3429, 2176, 118, 0, 2, 26, 723, 3657, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>157</td>\n",
       "      <td>5</td>\n",
       "      <td>Aggressive self-glorification and a manipulati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 3849, 0, 2, 0, 4633, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PhraseId  SentenceId  \\\n",
       "0           0         1           1   \n",
       "1           1        64           2   \n",
       "2           2        82           3   \n",
       "3           3       117           4   \n",
       "4           4       157           5   \n",
       "\n",
       "                                              review  sentiment5  sentiment  \\\n",
       "0  A series of escapades demonstrating the adage ...           1          0   \n",
       "1  This quiet , introspective and entertaining in...           4          1   \n",
       "2  Even fans of Ismail Merchant 's work , I suspe...           1          0   \n",
       "3  A positively thrilling combination of ethnogra...           3          1   \n",
       "4  Aggressive self-glorification and a manipulati...           1          0   \n",
       "\n",
       "                                             vectors  \n",
       "0  [0, 603, 118, 0, 0, 723, 0, 1544, 816, 1057, 4...  \n",
       "1    [2007, 4923, 0, 2, 692, 2667, 1057, 3497, 4748]  \n",
       "2  [2064, 752, 118, 0, 0, 0, 921, 0, 2984, 566, 7...  \n",
       "3  [0, 0, 3429, 2176, 118, 0, 2, 26, 723, 3657, 0...  \n",
       "4                        [0, 3849, 0, 2, 0, 4633, 0]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment5</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>281</td>\n",
       "      <td>7037</td>\n",
       "      <td>282</td>\n",
       "      <td>First , for a movie that tries to be smart , i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[4546, 3652, 0, 94, 1544, 2290, 493, 24, 1787,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3527</th>\n",
       "      <td>4384</td>\n",
       "      <td>84867</td>\n",
       "      <td>4389</td>\n",
       "      <td>A grand fart coming from a director beginning ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 4289, 0, 563, 980, 0, 151, 442, 493, 0, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>8027</td>\n",
       "      <td>147785</td>\n",
       "      <td>8041</td>\n",
       "      <td>Given the fact that virtually no one is bound ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[396, 723, 2274, 1544, 3260, 3018, 637, 1057, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>6389</td>\n",
       "      <td>119645</td>\n",
       "      <td>6399</td>\n",
       "      <td>There 's a lot of tooth in Roger Dodger .</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[1587, 0, 0, 751, 118, 338, 4398, 2788, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>5286</td>\n",
       "      <td>100741</td>\n",
       "      <td>5292</td>\n",
       "      <td>So aggressively cheery that Pollyana would rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[1809, 0, 0, 1544, 0, 566, 3785, 3652, 0, 0, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  PhraseId  SentenceId  \\\n",
       "228          281      7037         282   \n",
       "3527        4384     84867        4389   \n",
       "6462        8027    147785        8041   \n",
       "5140        6389    119645        6399   \n",
       "4265        5286    100741        5292   \n",
       "\n",
       "                                                 review  sentiment5  \\\n",
       "228   First , for a movie that tries to be smart , i...           1   \n",
       "3527  A grand fart coming from a director beginning ...           0   \n",
       "6462  Given the fact that virtually no one is bound ...           0   \n",
       "5140          There 's a lot of tooth in Roger Dodger .           3   \n",
       "4265  So aggressively cheery that Pollyana would rea...           1   \n",
       "\n",
       "      sentiment                                            vectors  \n",
       "228           0  [4546, 3652, 0, 94, 1544, 2290, 493, 24, 1787,...  \n",
       "3527          0  [0, 4289, 0, 563, 980, 0, 151, 442, 493, 0, 13...  \n",
       "6462          0  [396, 723, 2274, 1544, 3260, 3018, 637, 1057, ...  \n",
       "5140          1         [1587, 0, 0, 751, 118, 338, 4398, 2788, 0]  \n",
       "4265          0  [1809, 0, 0, 1544, 0, 566, 3785, 3652, 0, 0, 3...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataframe\n",
    "rotten_data = rotten_data.sample(frac=1)\n",
    "rotten_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3437, 7) (3437, 7)\n"
     ]
    }
   ],
   "source": [
    "# Split the data in half to get training and test sets. \n",
    "m = len(rotten_data) // 2\n",
    "rotten_train = rotten_data.iloc[:m]\n",
    "rotten_test = rotten_data.iloc[m:]\n",
    "print(rotten_train.shape, rotten_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Make a numpy file so we can easily load it into the other notebook\n",
    "helper.package_dataset(data_path + 'imdb.npz', np.array(train_data['vectors']), np.array(train_data['sentiment'], dtype='int16'), \n",
    "                                              np.array(test_data['vectors']), np.array(test_data['sentiment'], dtype='int16'))\n",
    "\n",
    "helper.package_dataset(data_path + 'rotten.npz', np.array(rotten_train['vectors']), np.array(rotten_train['sentiment'], dtype='int16'), \n",
    "                                              np.array(rotten_test['vectors']), np.array(rotten_test['sentiment'], dtype='int16'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# And that's it for preprocessing! Move on to the TVMLAI_CNN_LSTM for Part 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
